{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project-NLP.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "f6wyGly8-Vvo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to fruitfully process large amounts of natural language data (wikipedia). \n",
        "\n",
        "This rapidly improving area of artificial intelligence covers tasks such as speech recognition, natural-language understanding, and natural language generation.\n",
        "\n",
        "**Building a strong NLP foundation by practicing:**\n",
        "\n",
        "* Tokenizing - Splitting sentences and words from the body of text.\n",
        "* Part of Speech tagging\n",
        "* Chunking\n",
        "\n",
        "\n",
        "**Resources**\n",
        "\n",
        "Natural Language Toolkit (NLTK)"
      ]
    },
    {
      "metadata": {
        "id": "rlVXAK-9-Vvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk -q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5avxDc7v-Vv3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Downloading nltk packages**"
      ]
    },
    {
      "metadata": {
        "id": "5cc9u2Ia-Vv6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"udhr\")\n",
        "nltk.download(\"state_union\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Im-OwPcP-VwE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Basic natural language processing vocabulary:**\n",
        "\n",
        "##### Corpus - \n",
        "Body of text, singular. Corpora is the plural of this. Example: A collection of medical journals.\n",
        "##### Lexicon - \n",
        "Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons. \n",
        "##### Token - \n",
        "Each \"entity\" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph."
      ]
    },
    {
      "metadata": {
        "id": "x2cCdJf1-VwG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e9fc26a-8ba0-4e28-c160-755c5f9a7891"
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "text = \"Hello students, how are you doing today? The olympics are inspiring, and Python is awesome. You look nice today.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello students, how are you doing today?', 'The olympics are inspiring, and Python is awesome.', 'You look nice today.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wby2oOQ7-VwO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "a4caf3a5-c366-41f3-f693-74eba5156a13"
      },
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'students', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'olympics', 'are', 'inspiring', ',', 'and', 'Python', 'is', 'awesome', '.', 'You', 'look', 'nice', 'today', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zvDT8BK9-VwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Stop Words with NLTK:\n",
        "\n",
        "When using Natural Language Processing, our goal is to perform some analysis or processing so that a computer can respond to text appropriately. \n",
        "\n",
        "The process of converting data to something a computer can understand is referred to as \"pre-processing.\" One of the major forms of pre-processing is going to be filtering out useless data. In natural language processing, useless words (data), are referred to as stop words."
      ]
    },
    {
      "metadata": {
        "id": "YSxj3qxj-VwW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f67980a6-d403-41be-f8e4-e79e791c2b1c"
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(set(stopwords.words('english')))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set([u'all', u'just', u\"don't\", u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'don', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u\"should've\", u\"haven't\", u'do', u'them', u'his', u'very', u\"you've\", u'they', u'not', u'during', u'now', u'him', u'nor', u\"wasn't\", u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u\"won't\", u'where', u\"mustn't\", u\"isn't\", u'few', u'because', u\"you'd\", u'doing', u'some', u'hasn', u\"hasn't\", u'are', u'our', u'ourselves', u'out', u'what', u'for', u\"needn't\", u'below', u're', u'does', u\"shouldn't\", u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u\"mightn't\", u\"doesn't\", u'were', u'here', u'shouldn', u'hers', u\"aren't\", u'by', u'on', u'about', u'couldn', u'of', u\"wouldn't\", u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u\"hadn't\", u'mightn', u\"couldn't\", u'wasn', u'your', u\"you're\", u'from', u'her', u'their', u'aren', u\"it's\", u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u\"didn't\", u'but', u\"that'll\", u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u\"weren't\", u'these', u'up', u'will', u'while', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u\"shan't\", u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u\"you'll\", u'so', u'y', u\"she's\", u'the', u'having', u'once'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OQkBAxxh-Vwd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "40350484-9c86-415d-ccc9-b05dc262b44c"
      },
      "cell_type": "code",
      "source": [
        "example_sent = \"This is some sample text, showing off the stop words filtration.\"\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'some', 'sample', 'text', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'text', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X24nQLHH-Vwo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Stemming Words with NLTK:\n",
        "\n",
        "Stemming, which attempts to normalize sentences, is another preprocessing step that we can perform.  In the english language, different variations of words and sentences often having the same meaning.  Stemming is a way to account for these variations; furthermore, it will help us shorten the sentences and shorten our lookup.  For example, consider the following sentence:\n",
        "\n",
        "* I was taking a ride on my horse.\n",
        "* I was riding my horse.\n",
        "\n",
        "These sentences mean the same thing, as noted by the same tense (-ing) in each sentence; however, that isn't intuitively understood by the computer. To account for all the variations of words in the english language, we can use the Porter stemmer, which has been around since 1979."
      ]
    },
    {
      "metadata": {
        "id": "CXB4AJxK-Vwr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "67dac9b7-e93c-4ae1-f95d-00ff7cda97fb"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "example_words = [\"ride\",\"riding\",\"rider\",\"rides\"]\n",
        "\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ride\n",
            "ride\n",
            "rider\n",
            "ride\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LkdA40xK-Vw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "5ed8c2b2-d2b2-4ad1-b992-176973435595"
      },
      "cell_type": "code",
      "source": [
        "# Now lets try stemming an entire sentence!\n",
        "\n",
        "new_text = \"When riders are riding their horses, they often think of how cowboys rode horses.\"\n",
        "\n",
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "    print(ps.stem(w))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "when\n",
            "rider\n",
            "are\n",
            "ride\n",
            "their\n",
            "hors\n",
            ",\n",
            "they\n",
            "often\n",
            "think\n",
            "of\n",
            "how\n",
            "cowboy\n",
            "rode\n",
            "hors\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EXqxQEsY-Vw_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Part of Speech Tagging with NLTK\n",
        "\n",
        "Part of speech tagging means labeling words as nouns, verbs, adjectives, etc. Even better, NLTK can handle tenses! \n",
        "\n",
        "Words can be labeled using PunktSentenceTokenizer. This tokenizer is capable of unsupervised learning, so it can be trained on any body of text. "
      ]
    },
    {
      "metadata": {
        "id": "_KMaosna-VxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We can use documents from the nltk.corpus.  As an example, lets load the universal declaration of human rights.\n",
        "from nltk.corpus import udhr\n",
        "print(udhr.raw('English-Latin1'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pT65CU9a-VxP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Lets import some sample and training text - George Bush's 2005 and 2006 state of the union addresses. \n",
        "\n",
        "\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eFhZmgT7-VxU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now that we have some text, we can train the PunktSentenceTokenizer\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mW3Z7QUd-Vxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now lets tokenize the sample_text using our trained tokenizer\n",
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dAkn2enP-Vxf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This function will tag each tokenized word with a part of speech\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[:5]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            print(tagged)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "# The output is a list of tuples - the word with it's part of speech\n",
        "process_content()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k9Bfn9giIb8R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* CC coordinating conjunction\n",
        "* CD cardinal digit\n",
        "* DT determiner\n",
        "* EX existential there (like: “there is” … think of it like “there exists”)\n",
        "* FW foreign word\n",
        "* IN preposition/subordinating conjunction\n",
        "* JJ adjective ‘big’\n",
        "* JJR adjective, comparative ‘bigger’\n",
        "* JJS adjective, superlative ‘biggest’\n",
        "* LS list marker 1)\n",
        "* MD modal could, will\n",
        "* NN noun, singular ‘desk’\n",
        "* NNS noun plural ‘desks’\n",
        "* NNP proper noun, singular ‘Harrison’\n",
        "* NNPS proper noun, plural ‘Americans’\n",
        "* PDT predeterminer ‘all the kids’\n",
        "* POS possessive ending parent‘s\n",
        "* PRP personal pronoun I, he, she\n",
        "* PRP possessive pronoun my, his, hers\n",
        "* RB adverb very, silently,\n",
        "* RBR adverb, comparative better\n",
        "* RBS adverb, superlative best\n",
        "* RP particle give up\n",
        "* TO to go ‘to‘ the store.\n",
        "* UH interjection errrrrrrrm\n",
        "* VB verb, base form take\n",
        "* VBD verb, past tense took\n",
        "* VBG verb, gerund/present participle taking\n",
        "* VBN verb, past participle taken\n",
        "* VBP verb, sing. present, non-3d take\n",
        "* VBZ verb, 3rd person sing. present takes\n",
        "* WDT wh-determiner which\n",
        "* WP wh-pronoun who, what\n",
        "* WP possessive wh-pronoun whose\n",
        "*  WRB wh-abverb where, when"
      ]
    },
    {
      "metadata": {
        "id": "2eVyAupu-Vxr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Chunking with NLTK\n",
        "\n",
        "Now that each word has been tagged with a part of speech, we can move onto chunking: grouping the words into meaningful clusters.  The main goal of chunking is to group words into \"noun phrases\", which is a noun with any associated verbs, adjectives, or adverbs. \n",
        "\n",
        "The part of speech tags that were generated in the previous step will be combined with regular expressions, such as the following:"
      ]
    },
    {
      "metadata": {
        "id": "WcNtNYfT-Vxs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "+ = match 1 or more\n",
        "? = match 0 or 1 repetitions.\n",
        "* = match 0 or MORE repetitions\t  \n",
        ". = Any character except a new line\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wq2iBXH3-Vx0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            \n",
        "            # combine the part-of-speech tag with a regular expression\n",
        "            \n",
        "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            \n",
        "            # draw the chunks with nltk\n",
        "            # chunked.draw()     \n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "process_content()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m8_8HoBm-Vx6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The main line in question is:"
      ]
    },
    {
      "metadata": {
        "id": "c_5D2qCu-Vx9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-9yfvB0d-VyB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This line, broken down:"
      ]
    },
    {
      "metadata": {
        "id": "FbILmXHi-VyE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "<RB.?>* = \"0 or more of any tense of adverb,\" followed by: \n",
        "\n",
        "<VB.?>* = \"0 or more of any tense of verb,\" followed by: \n",
        "\n",
        "<NNP>+ = \"One or more proper nouns,\" followed by \n",
        "\n",
        "<NN>? = \"zero or one singular noun.\" \n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r43-p_xv-VyJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We can access the chunks, which are stored as an NLTK tree \n",
        "\n",
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            \n",
        "            # combine the part-of-speech tag with a regular expression\n",
        "            \n",
        "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            \n",
        "            # print(chunked)\n",
        "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
        "                print(subtree)\n",
        "            \n",
        "            # draw the chunks with nltk\n",
        "            # chunked.draw()     \n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "process_content()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cmivOXHk-VyN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Chinking with NLTK\n",
        "\n",
        "Sometimes there are words in the chunks that we don't won't, we can remove them using a process called chinking."
      ]
    },
    {
      "metadata": {
        "id": "96_UGszi-VyO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[5:]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            \n",
        "            # The main difference here is the }{, vs. the {}. This means we're removing \n",
        "            # from the chink one or more verbs, prepositions, determiners, or the word 'to'.\n",
        "\n",
        "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
        "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
        "\n",
        "            chunkParser = nltk.RegexpParser(chunkGram)\n",
        "            chunked = chunkParser.parse(tagged)\n",
        "            \n",
        "            # print(chunked)\n",
        "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
        "                print(subtree)\n",
        "\n",
        "            # chunked.draw()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "process_content()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j-IhHmd_-VyS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Named Entity Recognition with NLTK\n",
        "\n",
        "One of the most common forms of chunking in natural language processing is called \"Named Entity Recognition.\" NLTK is able to identify people, places, things, locations, monetary figures, and more.\n",
        "\n",
        "There are two major options with NLTK's named entity recognition: either recognize all named entities, or recognize named entities as their respective type, like people, places, locations, etc.\n",
        "\n",
        "Here, with the option of binary = True, this means either something is a named entity, or not. There will be no further detail."
      ]
    },
    {
      "metadata": {
        "id": "O763O_yC-VyS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[5:]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
        "            # namedEnt.draw()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "        \n",
        "process_content()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JsP_t8Wm-VyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Text Classification\n",
        "\n",
        "##### Text classification using NLTK\n",
        "\n",
        "Now that we have covered the basics of preprocessing for Natural Language Processing, we can move on to text classification using simple machine learning classification algorithms."
      ]
    },
    {
      "metadata": {
        "id": "54iWel0j-VyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "70868ce4-fc5f-4348-ebbd-837e7ecea84e"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# shuffle the documents\n",
        "random.shuffle(documents)\n",
        "\n",
        "print('Number of Documents: {}'.format(len(documents)))\n",
        "print('First Review: {}'.format(documents[1]))\n",
        "\n",
        "all_words = []\n",
        "for w in movie_reviews.words():\n",
        "    all_words.append(w.lower())\n",
        "\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "print('Most common words: {}'.format(all_words.most_common(15)))\n",
        "print('The word happy: {}'.format(all_words[\"happy\"]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Documents: 2000\n",
            "First Review: ([u'i', u\"'\", u'm', u'not', u'quite', u'sure', u'what', u'to', u'say', u'about', u'mars', u'attacks', u'!', u',', u'which', u'is', u'obviously', u'the', u'work', u'of', u'a', u'deranged', u'genius', u'.', u'when', u'tim', u'burton', u\"'\", u's', u'twisted', u'alien', u'invasion', u'comedy', u'really', u'works', u',', u'it', u\"'\", u's', u'breathtaking', u'and', u'hilarious', u'in', u'equal', u'measure', u'.', u'and', u'when', u'it', u'doesn', u\"'\", u't', u'work', u',', u'it', u\"'\", u's', u'just', u'dull', u'.', u'i', u\"'\", u'm', u'not', u'even', u'sure', u'it', u'works', u'more', u'often', u'than', u'it', u'doesn', u\"'\", u't', u',', u'but', u'where', u'it', u'counts', u'--', u'that', u'is', u',', u'when', u'this', u'gleefully', u'evil', u'invading', u'force', u'from', u'the', u'red', u'planet', u'gets', u'down', u'to', u'the', u'business', u'of', u'blasting', u'us', u'to', u'kingdom', u'come', u'--', u'mars', u'attacks', u'!', u'is', u'brilliant', u'.', u'mars', u'attacks', u'!', u'is', u'based', u'on', u'a', u'rather', u'unsavory', u'series', u'of', u'trading', u'cards', u'released', u'by', u'topps', u'in', u'the', u'1950s', u',', u'and', u'it', u'takes', u'its', u'cues', u'from', u'the', u'same', u'sources', u'as', u'this', u'summer', u\"'\", u's', u'independence', u'day', u'--', u'old', u'alien', u'invasion', u'flicks', u',', u'disaster', u'movies', u',', u'and', u'big', u'-', u'budget', u'special', u'effects', u'extravaganzas', u'.', u'but', u'unlike', u'independence', u'day', u',', u'which', u'was', u'a', u'painfully', u'middle', u'-', u'of', u'-', u'the', u'-', u'road', u'appeal', u'to', u'the', u'hearts', u',', u'minds', u'and', u'wallets', u'of', u'america', u',', u'mars', u'attacks', u'!', u'has', u'a', u'fully', u'developed', u'and', u'very', u'personal', u'sense', u'of', u'wonder', u'about', u'it', u'.', u'the', u'big', u'difference', u'is', u'that', u'while', u'independence', u'day', u'celebrated', u'the', u'resilience', u'of', u'human', u'beings', u',', u'mars', u'attacks', u'!', u'portrays', u'us', u'as', u'the', u'greedy', u'and', u'hapless', u'schmucks', u'that', u'we', u'are', u'.', u'the', u'title', u'sequence', u'is', u'just', u'splendid', u'--', u'an', u'incredible', u'swarm', u'of', u'flying', u'saucers', u'rises', u'out', u'of', u'the', u'canals', u'of', u'the', u'red', u'planet', u'and', u'then', u'storms', u'earthward', u'through', u'the', u'solar', u'system', u'in', u'formation', u'.', u'the', u'spinning', u'metal', u'saucers', u'are', u'dead', u'ringers', u'for', u'the', u'invading', u'forces', u'of', u'movies', u'past', u',', u'and', u'their', u'first', u'appearance', u'on', u'the', u'big', u'screen', u'in', u'combination', u'with', u'danny', u'elfman', u\"'\", u's', u'thundering', u',', u'theremin', u'-', u'driven', u'score', u'is', u'absolutely', u'jaw', u'-', u'dropping', u'.', u'it', u\"'\", u's', u'so', u'overwhelming', u'that', u',', u'i', u'swear', u'to', u'you', u',', u'i', u'had', u'trouble', u'breathing', u'.', u'the', u'movie', u'lays', u'low', u'for', u'the', u'next', u'40', u'minutes', u'.', u'we', u'get', u'a', u'glimpse', u'of', u'the', u'alien', u'leader', u',', u'resplendent', u'in', u'a', u'purple', u'-', u'sequined', u'cape', u',', u'when', u'a', u'television', u'message', u'is', u'broadcast', u'to', u'the', u'people', u'of', u'the', u'world', u'--', u'naturally', u',', u'the', u'only', u'way', u'to', u'communicate', u'with', u'earthlings', u'is', u'to', u'preempt', u'our', u'regularly', u'scheduled', u'programming', u'.', u'but', u'mostly', u',', u'the', u'first', u'act', u'is', u'spent', u'developing', u'characters', u',', u'setting', u'up', u'mildly', u'comic', u'situations', u',', u'and', u'drawing', u'a', u'quirky', u'but', u'dishearteningly', u'drab', u'picture', u'of', u'america', u'according', u'to', u'tim', u'burton', u'.', u'it', u\"'\", u's', u'a', u'good', u'thing', u'that', u'the', u'most', u'compelling', u'personality', u'in', u'the', u'movie', u'belongs', u'to', u'the', u'aliens', u',', u'because', u'burton', u\"'\", u's', u'just', u'not', u'interested', u'in', u'making', u'more', u'than', u'caricatures', u'out', u'of', u'his', u'human', u'stars', u'.', u'anyone', u'who', u'comes', u'to', u'mars', u'attacks', u'!', u'expecting', u'to', u'see', u'a', u'lot', u'of', u'one', u'favorite', u'actor', u'is', u'bound', u'to', u'be', u'disappointed', u'--', u'with', u'few', u'exceptions', u',', u'these', u'characters', u'are', u'dispatched', u'as', u'the', u'film', u'progresses', u'.', u'jack', u'nicholson', u'is', u'a', u'lot', u'of', u'fun', u'as', u'the', u'president', u'of', u'the', u'united', u'states', u'but', u'fizzles', u'as', u'a', u'sleazy', u'las', u'vegas', u'hotel', u'developer', u'.', u'rod', u'steiger', u'does', u'an', u'amusing', u'enough', u'take', u'on', u'dr', u'.', u'strangelove', u\"'\", u's', u'buck', u'turgidson', u',', u'glenn', u'close', u'brings', u'some', u'star', u'power', u'to', u'the', u'role', u'of', u'the', u'first', u'lady', u',', u'and', u'martin', u'short', u'is', u'a', u'presidential', u'aide', u'who', u'meets', u'the', u'martians', u'in', u'the', u'white', u'house', u\"'\", u's', u'\"', u'kennedy', u'room', u',', u'\"', u'a', u'secluded', u'nook', u'where', u'he', u'unwittingly', u'tries', u'to', u'seduce', u'an', u'alien', u'dressed', u'up', u'as', u'a', u'big', u'-', u'haired', u',', u'pointy', u'-', u'breasted', u'sexpot', u'(', u'lisa', u'marie', u',', u'the', u'burton', u'flame', u'who', u'played', u'vampira', u'in', u'ed', u'wood', u')', u'--', u'in', u'an', u'earlier', u'scene', u',', u'it', u\"'\", u's', u'made', u'clear', u'that', u'the', u'aliens', u'studied', u'human', u'sexuality', u'in', u'the', u'pages', u'of', u'an', u'old', u'issue', u'of', u'playboy', u'.', u'sarah', u'jessica', u'parker', u'hosts', u'her', u'own', u'tv', u'show', u'and', u'michael', u'j', u'.', u'fox', u'is', u'her', u'newsman', u'husband', u'.', u'the', u'octogenarian', u'sylvia', u'sidney', u'has', u'a', u'funny', u'part', u'as', u'the', u'old', u'woman', u'who', u'cackles', u',', u'\"', u'they', u'blew', u'up', u'congress', u'!', u'\"', u'and', u'plays', u'a', u'decisive', u'role', u'in', u'the', u'defeat', u'of', u'the', u'alien', u'menace', u'.', u'jim', u'brown', u'and', u'tom', u'jones', u'are', u'the', u'film', u\"'\", u's', u'vegas', u'-', u'based', u'heroes', u',', u'and', u'director', u'barbet', u'schroeder', u'(', u'reversal', u'of', u'fortune', u')', u'has', u'a', u'funny', u'cameo', u'as', u'the', u'unfortunate', u'french', u'president', u'.', u'also', u'on', u'hand', u'and', u'underutilized', u'are', u'annette', u'bening', u',', u'pierce', u'brosnan', u',', u'pam', u'grier', u',', u'lukas', u'haas', u'and', u'natalie', u'portman', u'(', u'the', u'latter', u'two', u'also', u'show', u'up', u'in', u'woody', u'allen', u\"'\", u's', u'everyone', u'says', u'i', u'love', u'you', u')', u'.', u'the', u'real', u'stars', u',', u'of', u'course', u',', u'are', u'the', u'computer', u'-', u'generated', u'martians', u',', u'and', u'they', u\"'\", u're', u'fantastic', u'.', u'with', u'bare', u'brains', u'glistening', u'atop', u'their', u'grinning', u'skull', u'-', u'faces', u',', u'round', u'egg', u'-', u'eyes', u'with', u'red', u'pupils', u'darting', u'this', u'way', u'and', u'that', u',', u'these', u'animated', u'invaders', u'are', u'malevolence', u'incarnate', u'--', u'joe', u'dante', u\"'\", u's', u'gremlins', u'are', u'the', u'closest', u'equivalent', u'in', u'recent', u'memory', u',', u'but', u'burton', u\"'\", u's', u'little', u'monsters', u'are', u'more', u'inventive', u'.', u'the', u'film', u\"'\", u's', u'raison', u'd', u\"'\", u'etre', u'are', u'the', u'scenes', u'of', u'crimes', u'against', u'mankind', u'--', u'the', u'most', u'famous', u'monuments', u'of', u'the', u'world', u'crumble', u'under', u'the', u'alien', u'assault', u',', u'easter', u'island', u'is', u'turned', u'into', u'a', u'bowling', u'alley', u',', u'and', u'the', u'nasty', u'little', u'buggers', u'perform', u'hideous', u'medical', u'experiments', u'on', u'captured', u'humans', u'.', u'the', u'sheer', u'level', u'of', u'mayhem', u'is', u'staggering', u',', u'especially', u'when', u'the', u'aliens', u'make', u'their', u'first', u'attack', u',', u'blasting', u'human', u'beings', u'into', u'iridescent', u'skeletons', u'.', u'parents', u'are', u'urged', u'to', u'pay', u'special', u'attention', u'to', u'the', u'pg', u'-', u'13', u'rating', u'--', u'it', u\"'\", u's', u'earned', u'.', u'the', u'special', u'effects', u'vary', u'from', u'charming', u'to', u'astonishing', u'to', u'deliciously', u'cheesy', u',', u'and', u'at', u'least', u'a', u'couple', u'of', u'shots', u'of', u'mass', u'destruction', u'seem', u'to', u'have', u'been', u'engineered', u'specifically', u'in', u'response', u'to', u'independence', u'day', u',', u'although', u'mars', u'attacks', u'!', u'was', u'in', u'production', u'long', u'before', u'that', u'film', u\"'\", u's', u'release', u'.', u'cinematographer', u'peter', u'suschitzky', u',', u'who', u'makes', u'the', u'most', u'of', u'wynn', u'thomas', u\"'\", u's', u'wildly', u'imaginative', u'production', u'design', u'(', u'thomas', u'helped', u'define', u'the', u'spike', u'lee', u'style', u')', u',', u'actually', u'took', u'time', u'off', u'from', u'his', u'mars', u'attacks', u'!', u'regimen', u'to', u'shoot', u'crash', u'for', u'david', u'cronenberg', u'.', u'cronenberg', u\"'\", u's', u'movie', u'will', u'almost', u'certainly', u'be', u'more', u'somber', u'--', u'it', u'won', u\"'\", u't', u'be', u'released', u'in', u'the', u'u', u'.', u's', u'.', u'until', u'early', u'next', u'year', u'--', u'but', u'mars', u'attacks', u'!', u'is', u'surely', u'a', u'similarly', u'elegant', u'nightmare', u'.', u'this', u'is', u'not', u'a', u'nice', u'movie', u'.', u'(', u'and', u'which', u'studio', u'executive', u'was', u'duped', u'into', u'giving', u'burton', u'$', u'70', u'million', u'to', u'make', u'it', u'?', u')', u'by', u'setting', u'the', u'film', u\"'\", u's', u'final', u'scenes', u'in', u'the', u'nearly', u'empty', u',', u'holocaust', u'-', u'ravaged', u'america', u'that', u'independence', u'day', u'conveniently', u'avoided', u',', u'burton', u\"'\", u's', u'invasion', u'epic', u'suggests', u'that', u'the', u'world', u'we', u\"'\", u've', u'got', u'may', u'not', u'be', u'worth', u'saving', u'until', u'many', u'of', u'the', u'people', u'on', u'it', u'are', u'dead', u'.', u'the', u'victims', u'of', u'his', u'invasion', u'are', u'naive', u',', u'greedy', u',', u'or', u'disastrously', u'self', u'-', u'absorbed', u'--', u'and', u'a', u'good', u'thing', u',', u'too', u',', u'since', u'the', u'movie', u\"'\", u's', u'wicked', u'charm', u'is', u'dependent', u'on', u'our', u'ability', u'to', u'take', u'this', u'invasion', u'partly', u'as', u'a', u'wish', u'-', u'fulfillment', u'fantasy', u'.', u'the', u'surviving', u'cast', u'members', u'are', u'the', u'innocents', u'and', u'the', u'entertainers', u',', u'who', u'triumph', u'by', u'determining', u'that', u'the', u'alien', u'invaders', u'--', u'the', u'ultimate', u'elitists', u'--', u'are', u'actually', u'susceptible', u'to', u'the', u'kitschiest', u'strains', u'of', u'american', u'pop', u'culture', u'.', u'ultimately', u',', u'this', u'messy', u'masterpiece', u'is', u'the', u'year', u\"'\", u's', u'funniest', u'comedy', u'and', u'a', u'weird', u',', u'winking', u'affirmation', u'of', u'the', u'power', u'of', u'the', u'people', u'.'], u'pos')\n",
            "Most common words: [(u',', 77717), (u'the', 76529), (u'.', 65876), (u'a', 38106), (u'and', 35576), (u'of', 34123), (u'to', 31937), (u\"'\", 30585), (u'is', 25195), (u'in', 21822), (u's', 18513), (u'\"', 17612), (u'it', 16107), (u'that', 15924), (u'-', 15595)]\n",
            "The word happy: 215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ctHN5srF-Vyj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e601b1c-919a-4904-9a8a-790fd48acda3"
      },
      "cell_type": "code",
      "source": [
        "# We'll use the 4000 most common words as features\n",
        "print(len(all_words))\n",
        "word_features = list(all_words.keys())[:4000]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RQCY-ZDe-Vyo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The find_features function will determine which of the 3000 word features are contained in the review\n",
        "def find_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Lets use an example from a negative review\n",
        "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
        "for key, value in features.items():\n",
        "    if value == True:\n",
        "        print key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZNx-HeR-Vys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Now lets do it for all the documents\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PIRrVRva-Vyx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# we can split the featuresets into training and testing datasets using sklearn\n",
        "from sklearn import model_selection\n",
        "\n",
        "# define a seed for reproducibility\n",
        "seed = 1\n",
        "\n",
        "# split the data into training and testing datasets\n",
        "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HVQWJgLI-Vy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "398eae96-d85b-4a8b-ed83-2648a87207dd"
      },
      "cell_type": "code",
      "source": [
        "print(len(training))\n",
        "print(len(testing))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1500\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DSP5eYrJ-Vy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d43bbfd-118c-4183-cba2-0967a4b5de9b"
      },
      "cell_type": "code",
      "source": [
        "# We can use sklearn algorithms in NLTK\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
        "\n",
        "# train the model on the training data\n",
        "model.train(training)\n",
        "\n",
        "# and test on the testing dataset!\n",
        "accuracy = nltk.classify.accuracy(model, testing)*100\n",
        "print(\"SVC Accuracy: {}\".format(accuracy))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC Accuracy: 64.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gvFyXL_Z-Vy_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Results and Summary\n",
        "\n",
        "In this project, we built a foundation for Natural Language Processing in Python. We covered tokenizing, stemming, part of speech tagging, chunking, named entity recognition, and text classification. "
      ]
    }
  ]
}